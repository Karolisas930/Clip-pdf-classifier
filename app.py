import os
import streamlit as st
import torch
import clip
from PIL import Image
import fitz # PyMuPDF
import pandas as pd
import numpy as np
from ocr_utils import extract_text_from_image

# â”€â”€ 1) Must be the very first Streamlit command â”€â”€
st.set_page_config(
    page_title="Smart PDF/Image Classifier",
    layout="wide",
)

st.title("ğŸ“„ğŸ” Smart PDF/Image Classifier with CLIP")

# â”€â”€ 2) Quick sanity check for NumPy â”€â”€
st.markdown(f"âœ… **NumPy** imported, version: `{np.__version__}`")

# â”€â”€ 3) Load & cache your hierarchy labels â”€â”€
uploaded = st.file_uploader(
    "Upload a PDF or image file",
    type=["pdf", "png", "jpg", "jpeg"],
    key="uploader_mixed",
)
if not uploaded:
    st.info("Please upload a PDF or image file to classify.")
    st.stop()

# â”€â”€ 4) Load CLIP + tokenize/encode your labels once â”€â”€
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MODEL, PREPROCESS = clip.load("ViT-B/32", device=DEVICE)

# truncate long labels so they fit in CLIPâ€™s 77â€token window
TEXT_TOKENS = clip.tokenize(LABELS, truncate=True).to(DEVICE)
with torch.no_grad():
    TEXT_FEATURES = MODEL.encode_text(TEXT_TOKENS)

# â”€â”€ 5) File uploader â”€â”€
uploaded = st.file_uploader(
    "Upload a PDF or image file",
    type=["pdf", "png", "jpg", "jpeg"],
    key="uploader_mixed",
)
if not uploaded:
    st.info("Please upload a PDF or image file to classify.")
    st.stop()

# â”€â”€ 6) PDF â†’ firstâ€page preview or image â†’ save as preview.png â”€â”€
if uploaded.type == "application/pdf":
    with open("temp.pdf", "wb") as f:
        f.write(uploaded.getbuffer())
    doc = fitz.open("temp.pdf")
    page = doc.load_page(0)
    pix = page.get_pixmap()
    image_path = "preview.png"
    pix.save(image_path)
else:
    image = Image.open(uploaded)
    image_path = "preview.png"
    image.save(image_path)

# â”€â”€ 7) Show it + OCR text â”€â”€
st.image(image_path, caption="Preview", use_container_width=True)
st.subheader("ğŸ“ Extracted OCR Text")
st.write(extract_text_from_image(image_path))

# â”€â”€ 8) Run CLIP & show top 5 â”€â”€
st.subheader("ğŸ”® Classification Results")
img_tensor = PREPROCESS(Image.open(image_path)).unsqueeze(0).to(DEVICE)
with torch.no_grad():
    image_features = MODEL.encode_image(img_tensor)
    logits = (image_features @ TEXT_FEATURES.T).softmax(dim=-1)
    probs = logits.cpu().numpy()[0]

for label, prob in sorted(zip(LABELS, probs), key=lambda x: x[1], reverse=True)[:5]:
    st.write(f"**{label}** â€” {prob:.2%}")
